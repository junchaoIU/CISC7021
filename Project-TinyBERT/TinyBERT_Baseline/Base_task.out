0

  Unit    5 Error on OPEN: md2.in                                                                                                                                                                                                                                                          
11/26 02:57:39 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/eval_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 02:57:39 PM device: cuda n_gpu: 1
11/26 02:57:39 PM Writing example 0 of 527
11/26 02:57:39 PM *** Example ***
11/26 02:57:39 PM guid: dev-0
11/26 02:57:39 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 02:57:39 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 02:57:39 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 02:57:39 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 02:57:39 PM label: 1
11/26 02:57:39 PM label_id: 1
11/26 02:57:39 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 02:57:40 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output/pytorch_model.bin
11/26 02:57:40 PM loading model...
11/26 02:57:40 PM done!
11/26 02:57:46 PM ***** Running evaluation *****
11/26 02:57:46 PM   Num examples = 527
11/26 02:57:46 PM   Batch size = 32
11/26 02:57:46 PM 開始Eval
11/26 02:57:46 PM 開始Eval
11/26 02:57:50 PM Eval結束，縂耗時3.947965145111084s，batch_num为17，平均耗时0.23223324383006377s
11/26 02:57:50 PM Eval結束，縂耗時4.169524431228638s
11/26 02:57:50 PM ***** Eval results *****
11/26 02:57:50 PM   eval_loss = 0.9251355160685146
11/26 02:57:50 PM   mcc = 0.5251347112164407
11/26 03:00:04 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/eval_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:00:04 PM device: cuda n_gpu: 1
11/26 03:00:04 PM Writing example 0 of 527
11/26 03:00:04 PM *** Example ***
11/26 03:00:04 PM guid: dev-0
11/26 03:00:04 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:00:04 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:00:04 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:00:04 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:00:04 PM label: 1
11/26 03:00:04 PM label_id: 1
11/26 03:00:05 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:00:06 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output/pytorch_model.bin
11/26 03:00:06 PM loading model...
11/26 03:00:06 PM done!
11/26 03:00:16 PM ***** Running evaluation *****
11/26 03:00:16 PM   Num examples = 527
11/26 03:00:16 PM   Batch size = 32
11/26 03:00:16 PM 開始Eval
11/26 03:00:16 PM 開始Eval
11/26 03:00:22 PM Eval結束，縂耗時5.8888585567474365s，batch_num为17，平均耗时0.34640344451455507s
11/26 03:00:22 PM Eval結束，縂耗時6.12793231010437s
11/26 03:00:22 PM ***** Eval results *****
11/26 03:00:22 PM   eval_loss = 0.9251355160685146
11/26 03:00:22 PM   mcc = 0.5251347112164407
11/26 03:03:24 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/eval_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:03:24 PM device: cuda n_gpu: 1
11/26 03:03:24 PM Writing example 0 of 527
11/26 03:03:24 PM *** Example ***
11/26 03:03:24 PM guid: dev-0
11/26 03:03:24 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:03:24 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:03:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:03:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:03:24 PM label: 1
11/26 03:03:24 PM label_id: 1
11/26 03:03:25 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:03:25 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output/pytorch_model.bin
11/26 03:03:25 PM loading model...
11/26 03:03:25 PM done!
11/26 03:03:34 PM ***** Running evaluation *****
11/26 03:03:34 PM   Num examples = 527
11/26 03:03:34 PM   Batch size = 32
11/26 03:03:34 PM 開始Eval
11/26 03:03:34 PM 開始Eval
11/26 03:03:40 PM Eval結束，縂耗時5.619202136993408s，batch_num为17，平均耗时0.33054130217608285s
11/26 03:03:40 PM Eval結束，縂耗時5.79118537902832s
11/26 03:03:40 PM ***** Eval results *****
11/26 03:03:40 PM   eval_loss = 0.9251355160685146
11/26 03:03:40 PM   mcc = 0.5251347112164407
11/26 03:06:30 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/eval_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:06:31 PM device: cuda n_gpu: 1
11/26 03:06:31 PM Writing example 0 of 527
11/26 03:06:31 PM *** Example ***
11/26 03:06:31 PM guid: dev-0
11/26 03:06:31 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:06:31 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:06:31 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:06:31 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:06:31 PM label: 1
11/26 03:06:31 PM label_id: 1
11/26 03:06:31 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:06:32 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output/pytorch_model.bin
11/26 03:06:32 PM loading model...
11/26 03:06:32 PM done!
11/26 03:06:42 PM ***** Running evaluation *****
11/26 03:06:42 PM   Num examples = 527
11/26 03:06:42 PM   Batch size = 32
11/26 03:06:42 PM 開始Eval
11/26 03:06:42 PM 開始Eval
11/26 03:06:48 PM Eval結束，縂耗時5.773771047592163s，batch_num为17，平均耗时0.33963359103483315s
11/26 03:06:48 PM Eval結束，縂耗時6.006404161453247s
11/26 03:06:48 PM ***** Eval results *****
11/26 03:06:48 PM   eval_loss = 0.9251355160685146
11/26 03:06:48 PM   mcc = 0.5251347112164407
11/26 03:09:50 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT/eval_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:09:50 PM device: cuda n_gpu: 1
11/26 03:09:51 PM Writing example 0 of 527
11/26 03:09:51 PM *** Example ***
11/26 03:09:51 PM guid: dev-0
11/26 03:09:51 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:09:51 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:09:51 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:09:51 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:09:51 PM label: 1
11/26 03:09:51 PM label_id: 1
11/26 03:09:51 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:09:52 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT/Base_output/pytorch_model.bin
11/26 03:09:52 PM loading model...
11/26 03:09:52 PM done!
11/26 03:10:02 PM ***** Running evaluation *****
11/26 03:10:02 PM   Num examples = 527
11/26 03:10:02 PM   Batch size = 32
11/26 03:10:02 PM 開始Eval
11/26 03:10:02 PM 開始Eval
11/26 03:10:09 PM Eval結束，縂耗時7.152732610702515s，batch_num为17，平均耗时0.4207489771001479s
11/26 03:10:09 PM Eval結束，縂耗時7.4829559326171875s
11/26 03:10:09 PM ***** Eval results *****
11/26 03:10:09 PM   eval_loss = 0.9251355160685146
11/26 03:10:09 PM   mcc = 0.5251347112164407
