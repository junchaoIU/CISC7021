1

  Unit    5 Error on OPEN: md2.in                                                                                                                                                                                                                                                          
11/26 03:00:19 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/eval_B_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:00:19 PM device: cuda n_gpu: 1
11/26 03:00:19 PM Writing example 0 of 527
11/26 03:00:19 PM *** Example ***
11/26 03:00:19 PM guid: dev-0
11/26 03:00:19 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:00:19 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:00:19 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:00:19 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:00:20 PM label: 1
11/26 03:00:20 PM label_id: 1
11/26 03:00:20 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:00:20 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2/pytorch_model.bin
11/26 03:00:20 PM loading model...
11/26 03:00:20 PM done!
11/26 03:00:23 PM ***** Running evaluation *****
11/26 03:00:23 PM   Num examples = 527
11/26 03:00:23 PM   Batch size = 32
11/26 03:00:23 PM 開始Eval
11/26 03:00:23 PM 開始Eval
11/26 03:00:24 PM Eval結束，縂耗時0.674565315246582s，batch_num为17，平均耗时0.039680312661563646s
11/26 03:00:24 PM Eval結束，縂耗時0.8949706554412842s
11/26 03:00:24 PM ***** Eval results *****
11/26 03:00:24 PM   eval_loss = 0.9652249497525832
11/26 03:00:24 PM   mcc = 0.5026121718009313
11/26 03:03:24 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/eval_B_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:03:24 PM device: cuda n_gpu: 1
11/26 03:03:24 PM Writing example 0 of 527
11/26 03:03:24 PM *** Example ***
11/26 03:03:24 PM guid: dev-0
11/26 03:03:24 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:03:24 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:03:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:03:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:03:24 PM label: 1
11/26 03:03:24 PM label_id: 1
11/26 03:03:25 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:03:25 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2/pytorch_model.bin
11/26 03:03:25 PM loading model...
11/26 03:03:26 PM done!
11/26 03:03:34 PM ***** Running evaluation *****
11/26 03:03:34 PM   Num examples = 527
11/26 03:03:34 PM   Batch size = 32
11/26 03:03:34 PM 開始Eval
11/26 03:03:34 PM 開始Eval
11/26 03:03:40 PM Eval結束，縂耗時5.93777060508728s，batch_num为17，平均耗时0.34928062382866354s
11/26 03:03:40 PM Eval結束，縂耗時6.160688400268555s
11/26 03:03:40 PM ***** Eval results *****
11/26 03:03:40 PM   eval_loss = 0.9652249497525832
11/26 03:03:40 PM   mcc = 0.5026121718009313
11/26 03:06:31 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/eval_B_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:06:31 PM device: cuda n_gpu: 1
11/26 03:06:31 PM Writing example 0 of 527
11/26 03:06:31 PM *** Example ***
11/26 03:06:31 PM guid: dev-0
11/26 03:06:31 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:06:31 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:06:31 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:06:31 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:06:31 PM label: 1
11/26 03:06:31 PM label_id: 1
11/26 03:06:31 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:06:32 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2/pytorch_model.bin
11/26 03:06:32 PM loading model...
11/26 03:06:32 PM done!
11/26 03:06:42 PM ***** Running evaluation *****
11/26 03:06:42 PM   Num examples = 527
11/26 03:06:42 PM   Batch size = 32
11/26 03:06:42 PM 開始Eval
11/26 03:06:42 PM 開始Eval
11/26 03:06:48 PM Eval結束，縂耗時5.900601387023926s，batch_num为17，平均耗时0.34709419923670154s
11/26 03:06:48 PM Eval結束，縂耗時6.143865346908569s
11/26 03:06:48 PM ***** Eval results *****
11/26 03:06:48 PM   eval_loss = 0.9652249497525832
11/26 03:06:48 PM   mcc = 0.5026121718009313
11/26 03:09:50 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/eval_B_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:09:50 PM device: cuda n_gpu: 1
11/26 03:09:51 PM Writing example 0 of 527
11/26 03:09:51 PM *** Example ***
11/26 03:09:51 PM guid: dev-0
11/26 03:09:51 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:09:51 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:09:51 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:09:51 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:09:51 PM label: 1
11/26 03:09:51 PM label_id: 1
11/26 03:09:51 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:09:52 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2/pytorch_model.bin
11/26 03:09:52 PM loading model...
11/26 03:09:52 PM done!
11/26 03:10:02 PM ***** Running evaluation *****
11/26 03:10:02 PM   Num examples = 527
11/26 03:10:02 PM   Batch size = 32
11/26 03:10:02 PM 開始Eval
11/26 03:10:02 PM 開始Eval
11/26 03:10:09 PM Eval結束，縂耗時7.264350175857544s，batch_num为17，平均耗时0.42731471622691436s
11/26 03:10:09 PM Eval結束，縂耗時7.549234390258789s
11/26 03:10:09 PM ***** Eval results *****
11/26 03:10:09 PM   eval_loss = 0.9652249497525832
11/26 03:10:09 PM   mcc = 0.5026121718009313
11/26 03:12:50 PM The args: Namespace(data_dir='CoLA', teacher_model=None, student_model='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2', task_name='CoLA', output_dir='/data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/eval_B_output', cache_dir='', max_seq_length=64, do_eval=True, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=3.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
11/26 03:12:50 PM device: cuda n_gpu: 1
11/26 03:12:50 PM Writing example 0 of 527
11/26 03:12:50 PM *** Example ***
11/26 03:12:50 PM guid: dev-0
11/26 03:12:50 PM tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
11/26 03:12:50 PM input_ids: 101 1103 12428 8335 1103 10647 2330 1104 1103 5753 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:12:50 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:12:50 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/26 03:12:50 PM label: 1
11/26 03:12:50 PM label_id: 1
11/26 03:12:51 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1248,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pooler_fc_size": 312,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 28996
}

11/26 03:12:51 PM Loading model /data/home/mc25653/Pretrained-Language-Model/TinyBERT_B/tinybert_without_B2/pytorch_model.bin
11/26 03:12:51 PM loading model...
11/26 03:12:51 PM done!
11/26 03:13:00 PM ***** Running evaluation *****
11/26 03:13:00 PM   Num examples = 527
11/26 03:13:00 PM   Batch size = 32
11/26 03:13:00 PM 開始Eval
11/26 03:13:00 PM 開始Eval
11/26 03:13:07 PM Eval結束，縂耗時7.06128454208374s，batch_num为17，平均耗时0.4153696789461024s
11/26 03:13:07 PM Eval結束，縂耗時7.3271472454071045s
11/26 03:13:07 PM ***** Eval results *****
11/26 03:13:07 PM   eval_loss = 0.9652249497525832
11/26 03:13:07 PM   mcc = 0.5026121718009313
