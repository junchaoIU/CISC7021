nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/wujunchao/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sat Nov  5 21:02:54 CST 2022
Executing: mkdir -p /home/wujunchao/mosesdecoder/corpus/working/train/corpus
(1.0) selecting factors @ Sat Nov  5 21:02:54 CST 2022
(1.1) running mkcls  @ Sat Nov  5 21:02:54 CST 2022
/home/wujunchao/mosesdecoder/tools/mkcls -c50 -n2 -p/home/wujunchao/mosesdecoder/corpus/clean_train.zh -V/home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb.classes opt
Executing: /home/wujunchao/mosesdecoder/tools/mkcls -c50 -n2 -p/home/wujunchao/mosesdecoder/corpus/clean_train.zh -V/home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 53424

start-costs: MEAN: 4.70397e+07 (4.69898e+07-4.70895e+07)  SIGMA:49851.1   
  end-costs: MEAN: 4.50932e+07 (4.50865e+07-4.50999e+07)  SIGMA:6717.67   
   start-pp: MEAN: 737.327 (725.733-748.921)  SIGMA:11.5942   
     end-pp: MEAN: 398.963 (398.117-399.808)  SIGMA:0.845458   
 iterations: MEAN: 1.43561e+06 (1.3552e+06-1.51601e+06)  SIGMA:80407   
       time: MEAN: 23.9086 (21.7108-26.1064)  SIGMA:2.19779   
(1.1) running mkcls  @ Sat Nov  5 21:03:51 CST 2022
/home/wujunchao/mosesdecoder/tools/mkcls -c50 -n2 -p/home/wujunchao/mosesdecoder/corpus/clean_train.en -V/home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb.classes opt
Executing: /home/wujunchao/mosesdecoder/tools/mkcls -c50 -n2 -p/home/wujunchao/mosesdecoder/corpus/clean_train.en -V/home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 56636

start-costs: MEAN: 4.95011e+07 (4.9382e+07-4.96202e+07)  SIGMA:119113   
  end-costs: MEAN: 4.68232e+07 (4.6818e+07-4.68284e+07)  SIGMA:5165.48   
   start-pp: MEAN: 415.826 (401.054-430.599)  SIGMA:14.7726   
     end-pp: MEAN: 186.907 (186.619-187.195)  SIGMA:0.288073   
 iterations: MEAN: 1.38683e+06 (1.38141e+06-1.39225e+06)  SIGMA:5417.5   
       time: MEAN: 19.6506 (19.632-19.6692)  SIGMA:0.018624   
(1.2) creating vcb file /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb @ Sat Nov  5 21:04:33 CST 2022
(1.2) creating vcb file /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb @ Sat Nov  5 21:04:34 CST 2022
(1.3) numberizing corpus /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt @ Sat Nov  5 21:04:34 CST 2022
(1.3) numberizing corpus /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt @ Sat Nov  5 21:04:36 CST 2022
(2) running giza @ Sat Nov  5 21:04:38 CST 2022
(2.1a) running snt2cooc zh-en @ Sat Nov  5 21:04:38 CST 2022

Executing: mkdir -p /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en
Executing: /home/wujunchao/mosesdecoder/tools/snt2cooc.out /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt > /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.cooc
/home/wujunchao/mosesdecoder/tools/snt2cooc.out /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt > /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
line 86000
line 87000
line 88000
line 89000
line 90000
line 91000
line 92000
line 93000
line 94000
line 95000
line 96000
line 97000
line 98000
line 99000
line 100000
line 101000
line 102000
line 103000
line 104000
line 105000
line 106000
line 107000
line 108000
line 109000
line 110000
line 111000
line 112000
line 113000
line 114000
line 115000
line 116000
line 117000
line 118000
line 119000
line 120000
line 121000
line 122000
line 123000
line 124000
line 125000
line 126000
line 127000
line 128000
line 129000
line 130000
line 131000
line 132000
line 133000
line 134000
line 135000
line 136000
line 137000
line 138000
line 139000
line 140000
line 141000
line 142000
line 143000
line 144000
line 145000
line 146000
line 147000
line 148000
line 149000
line 150000
line 151000
line 152000
line 153000
line 154000
line 155000
line 156000
line 157000
line 158000
line 159000
line 160000
line 161000
line 162000
line 163000
line 164000
line 165000
line 166000
line 167000
line 168000
line 169000
line 170000
line 171000
line 172000
line 173000
END.
(2.1b) running giza zh-en @ Sat Nov  5 21:05:02 CST 2022
/home/wujunchao/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.cooc -c /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en -onlyaldumps 1 -p0 0.999 -s /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb -t /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb
Executing: /home/wujunchao/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.cooc -c /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en -onlyaldumps 1 -p0 0.999 -s /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb -t /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb
/home/wujunchao/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.cooc -c /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en -onlyaldumps 1 -p0 0.999 -s /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb -t /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb
Parameter 'coocurrencefile' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.cooc'
Parameter 'c' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2022-11-05.210503.wujunchao' to '/home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2022-11-05.210503.wujunchao.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2022-11-05.210503.wujunchao.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb
Reading vocabulary file from:/home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb
Source vocabulary list has 56637 unique tokens 
Target vocabulary list has 53425 unique tokens 
Calculating vocabulary frequencies from corpus /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh-en-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 173918
Size of source portion of the training corpus: 3.17753e+06 tokens
Size of the target portion of the training corpus: 2.99608e+06 tokens 
In source portion of the training corpus, only 56636 unique tokens appeared
In target portion of the training corpus, only 53423 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 2.99608e+06/(3.35145e+06-173918)== 0.942895
There are 10514129 10514129 entries in table
==========================================================
Model1 Training Started at: Sat Nov  5 21:05:04 2022

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 15.9483 PERPLEXITY 63227.8
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 20.4093 PERPLEXITY 1.39257e+06
Model 1 Iteration: 1 took: 11 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 8.05206 PERPLEXITY 265.407
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 10.1957 PERPLEXITY 1172.8
Model 1 Iteration: 2 took: 13 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 7.08627 PERPLEXITY 135.887
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.67014 PERPLEXITY 407.353
Model 1 Iteration: 3 took: 13 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 6.77929 PERPLEXITY 109.843
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 8.05057 PERPLEXITY 265.132
Model 1 Iteration: 4 took: 12 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 6.67162 PERPLEXITY 101.943
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.76812 PERPLEXITY 217.991
Model 1 Iteration: 5 took: 12 seconds
Entire Model1 Training took: 61 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 56636  #classes: 51
Read classes: #words: 53424  #classes: 51

==========================================================
Hmm Training Started at: Sat Nov  5 21:06:05 2022

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 64040 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 6.62445 PERPLEXITY 98.6638
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.61325 PERPLEXITY 195.802

Hmm Iteration: 1 took: 62 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 64040 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 6.26408 PERPLEXITY 76.8558
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.73213 PERPLEXITY 106.309

Hmm Iteration: 2 took: 50 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 64040 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 5.92608 PERPLEXITY 60.8033
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 6.24305 PERPLEXITY 75.7437

Hmm Iteration: 3 took: 50 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 64040 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 5.76802 PERPLEXITY 54.4937
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 6.02053 PERPLEXITY 64.9172

Hmm Iteration: 4 took: 65 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 64040 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 5.69253 PERPLEXITY 51.7158
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 5.9131 PERPLEXITY 60.2587

Hmm Iteration: 5 took: 72 seconds

Entire Hmm Training took: 299 seconds
==========================================================
Read classes: #words: 56636  #classes: 51
Read classes: #words: 53424  #classes: 51
Read classes: #words: 56636  #classes: 51
Read classes: #words: 53424  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Nov  5 21:11:05 2022


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 583.407 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 64040 parameters.
A/D table contains 61936 parameters.
NTable contains 566370 parameter.
p0_count is 2.2545e+06 and p1 is 370735; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 5.66561 PERPLEXITY 50.7595
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 5.80849 PERPLEXITY 56.0439

THTo3 Viterbi Iteration : 1 took: 49 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 585.6 #alsophisticatedcountcollection: 0 #hcsteps: 3.63058
#peggingImprovements: 0
A/D table contains 64040 parameters.
A/D table contains 61935 parameters.
NTable contains 566370 parameter.
p0_count is 2.69932e+06 and p1 is 148380; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 7.29227 PERPLEXITY 156.744
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 7.41156 PERPLEXITY 170.256

Model3 Viterbi Iteration : 2 took: 43 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 585.738 #alsophisticatedcountcollection: 0 #hcsteps: 3.59589
#peggingImprovements: 0
A/D table contains 64040 parameters.
A/D table contains 61935 parameters.
NTable contains 566370 parameter.
p0_count is 2.79359e+06 and p1 is 101242; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 7.13583 PERPLEXITY 140.636
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 7.24259 PERPLEXITY 151.438

Model3 Viterbi Iteration : 3 took: 43 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 585.742 #alsophisticatedcountcollection: 79.2535 #hcsteps: 3.55641
#peggingImprovements: 0
D4 table contains 527800 parameters.
A/D table contains 64040 parameters.
A/D table contains 61935 parameters.
NTable contains 566370 parameter.
p0_count is 2.81777e+06 and p1 is 89154.3; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 7.09849 PERPLEXITY 137.043
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 7.20044 PERPLEXITY 147.078

T3To4 Viterbi Iteration : 4 took: 57 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 585.745 #alsophisticatedcountcollection: 58.1802 #hcsteps: 2.966
#peggingImprovements: 0
D4 table contains 527800 parameters.
A/D table contains 64040 parameters.
A/D table contains 61965 parameters.
NTable contains 566370 parameter.
p0_count is 2.78555e+06 and p1 is 105261; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.93395 PERPLEXITY 122.272
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 7.00872 PERPLEXITY 128.776

Model4 Viterbi Iteration : 5 took: 110 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 585.724 #alsophisticatedcountcollection: 52.7863 #hcsteps: 2.91818
#peggingImprovements: 0
D4 table contains 527800 parameters.
A/D table contains 64040 parameters.
A/D table contains 61989 parameters.
NTable contains 566370 parameter.
p0_count is 2.78545e+06 and p1 is 105315; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.77207 PERPLEXITY 109.294
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 6.84166 PERPLEXITY 114.695

Model4 Viterbi Iteration : 6 took: 115 seconds
H333444 Training Finished at: Sat Nov  5 21:18:02 2022


Entire Viterbi H333444 Training took: 417 seconds
==========================================================

Entire Training took: 779 seconds
Program Finished at: Sat Nov  5 21:18:02 2022

==========================================================
Executing: rm -f /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.A3.final.gz
Executing: gzip /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.A3.final
(2.1a) running snt2cooc en-zh @ Sat Nov  5 21:18:05 CST 2022

Executing: mkdir -p /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh
Executing: /home/wujunchao/mosesdecoder/tools/snt2cooc.out /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt > /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.cooc
/home/wujunchao/mosesdecoder/tools/snt2cooc.out /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt > /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
line 29000
line 30000
line 31000
line 32000
line 33000
line 34000
line 35000
line 36000
line 37000
line 38000
line 39000
line 40000
line 41000
line 42000
line 43000
line 44000
line 45000
line 46000
line 47000
line 48000
line 49000
line 50000
line 51000
line 52000
line 53000
line 54000
line 55000
line 56000
line 57000
line 58000
line 59000
line 60000
line 61000
line 62000
line 63000
line 64000
line 65000
line 66000
line 67000
line 68000
line 69000
line 70000
line 71000
line 72000
line 73000
line 74000
line 75000
line 76000
line 77000
line 78000
line 79000
line 80000
line 81000
line 82000
line 83000
line 84000
line 85000
line 86000
line 87000
line 88000
line 89000
line 90000
line 91000
line 92000
line 93000
line 94000
line 95000
line 96000
line 97000
line 98000
line 99000
line 100000
line 101000
line 102000
line 103000
line 104000
line 105000
line 106000
line 107000
line 108000
line 109000
line 110000
line 111000
line 112000
line 113000
line 114000
line 115000
line 116000
line 117000
line 118000
line 119000
line 120000
line 121000
line 122000
line 123000
line 124000
line 125000
line 126000
line 127000
line 128000
line 129000
line 130000
line 131000
line 132000
line 133000
line 134000
line 135000
line 136000
line 137000
line 138000
line 139000
line 140000
line 141000
line 142000
line 143000
line 144000
line 145000
line 146000
line 147000
line 148000
line 149000
line 150000
line 151000
line 152000
line 153000
line 154000
line 155000
line 156000
line 157000
line 158000
line 159000
line 160000
line 161000
line 162000
line 163000
line 164000
line 165000
line 166000
line 167000
line 168000
line 169000
line 170000
line 171000
line 172000
line 173000
END.
(2.1b) running giza en-zh @ Sat Nov  5 21:18:35 CST 2022
/home/wujunchao/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.cooc -c /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh -onlyaldumps 1 -p0 0.999 -s /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb -t /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb
Executing: /home/wujunchao/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.cooc -c /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh -onlyaldumps 1 -p0 0.999 -s /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb -t /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb
/home/wujunchao/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.cooc -c /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh -onlyaldumps 1 -p0 0.999 -s /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb -t /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.cooc'
Parameter 'c' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2022-11-05.211835.wujunchao' to '/home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb'
Parameter 't' changed from '' to '/home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2022-11-05.211835.wujunchao.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb  (source vocabulary file name)
t = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2022-11-05.211835.wujunchao.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb  (source vocabulary file name)
t = /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/wujunchao/mosesdecoder/corpus/working/train/corpus/zh.vcb
Reading vocabulary file from:/home/wujunchao/mosesdecoder/corpus/working/train/corpus/en.vcb
Source vocabulary list has 53425 unique tokens 
Target vocabulary list has 56637 unique tokens 
Calculating vocabulary frequencies from corpus /home/wujunchao/mosesdecoder/corpus/working/train/corpus/en-zh-int-train.snt
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
 Train total # sentence pairs (weighted): 173918
Size of source portion of the training corpus: 2.99608e+06 tokens
Size of the target portion of the training corpus: 3.17753e+06 tokens 
In source portion of the training corpus, only 53424 unique tokens appeared
In target portion of the training corpus, only 56635 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 3.17753e+06/(3.17e+06-173918)== 1.06056
There are 10517341 10517341 entries in table
==========================================================
Model1 Training Started at: Sat Nov  5 21:18:37 2022

-----------
Model1: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (1) TRAIN CROSS-ENTROPY 16.0093 PERPLEXITY 65961.8
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 20.38 PERPLEXITY 1.36454e+06
Model 1 Iteration: 1 took: 10 seconds
-----------
Model1: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (2) TRAIN CROSS-ENTROPY 7.60693 PERPLEXITY 194.947
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.95 PERPLEXITY 989.12
Model 1 Iteration: 2 took: 10 seconds
-----------
Model1: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (3) TRAIN CROSS-ENTROPY 6.73774 PERPLEXITY 106.724
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.51611 PERPLEXITY 366.104
Model 1 Iteration: 3 took: 10 seconds
-----------
Model1: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (4) TRAIN CROSS-ENTROPY 6.46649 PERPLEXITY 88.4314
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.91691 PERPLEXITY 241.672
Model 1 Iteration: 4 took: 12 seconds
-----------
Model1: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Model1: (5) TRAIN CROSS-ENTROPY 6.37071 PERPLEXITY 82.7512
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.63332 PERPLEXITY 198.545
Model 1 Iteration: 5 took: 12 seconds
Entire Model1 Training took: 54 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 53424  #classes: 51
Read classes: #words: 56636  #classes: 51

==========================================================
Hmm Training Started at: Sat Nov  5 21:19:31 2022

-----------
Hmm: Iteration 1
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 62925 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 6.32759 PERPLEXITY 80.3148
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.47384 PERPLEXITY 177.767

Hmm Iteration: 1 took: 52 seconds

-----------
Hmm: Iteration 2
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 62925 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 6.09799 PERPLEXITY 68.4979
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.72184 PERPLEXITY 105.554

Hmm Iteration: 2 took: 75 seconds

-----------
Hmm: Iteration 3
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 62925 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 5.78981 PERPLEXITY 55.323
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 6.22014 PERPLEXITY 74.5504

Hmm Iteration: 3 took: 198 seconds

-----------
Hmm: Iteration 4
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 62925 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 5.63449 PERPLEXITY 49.6765
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.98426 PERPLEXITY 63.3056

Hmm Iteration: 4 took: 202 seconds

-----------
Hmm: Iteration 5
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
[sent:100000]
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
Reading more sentence pairs into memory ... 
A/D table contains 62925 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 5.55498 PERPLEXITY 47.0127
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 5.86471 PERPLEXITY 58.2713

Hmm Iteration: 5 took: 183 seconds

Entire Hmm Training took: 710 seconds
==========================================================
Read classes: #words: 53424  #classes: 51
Read classes: #words: 56636  #classes: 51
Read classes: #words: 53424  #classes: 51
Read classes: #words: 56636  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Nov  5 21:31:21 2022


---------------------
THTo3: Iteration 1
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 604.07 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 62925 parameters.
A/D table contains 63068 parameters.
NTable contains 534250 parameter.
p0_count is 2.18852e+06 and p1 is 494248; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 5.52467 PERPLEXITY 46.0355
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 5.66063 PERPLEXITY 50.5847

THTo3 Viterbi Iteration : 1 took: 132 seconds

---------------------
Model3: Iteration 2
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 608.249 #alsophisticatedcountcollection: 0 #hcsteps: 4.75321
#peggingImprovements: 0
A/D table contains 62925 parameters.
A/D table contains 63067 parameters.
NTable contains 534250 parameter.
p0_count is 2.9339e+06 and p1 is 121814; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 7.19687 PERPLEXITY 146.715
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 7.32742 PERPLEXITY 160.61

Model3 Viterbi Iteration : 2 took: 57 seconds

---------------------
Model3: Iteration 3
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 608.213 #alsophisticatedcountcollection: 0 #hcsteps: 4.66939
#peggingImprovements: 0
A/D table contains 62925 parameters.
A/D table contains 63067 parameters.
NTable contains 534250 parameter.
p0_count is 3.00414e+06 and p1 is 86695.9; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 7.04078 PERPLEXITY 131.67
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 7.15767 PERPLEXITY 142.782

Model3 Viterbi Iteration : 3 took: 57 seconds

---------------------
T3To4: Iteration 4
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 608.224 #alsophisticatedcountcollection: 97.2791 #hcsteps: 4.56346
#peggingImprovements: 0
D4 table contains 527800 parameters.
A/D table contains 62925 parameters.
A/D table contains 63067 parameters.
NTable contains 534250 parameter.
p0_count is 3.01293e+06 and p1 is 82299.3; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 7.00739 PERPLEXITY 128.658
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 7.11856 PERPLEXITY 138.963

T3To4 Viterbi Iteration : 4 took: 84 seconds

---------------------
Model4: Iteration 5
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 608.276 #alsophisticatedcountcollection: 73.65 #hcsteps: 3.86761
#peggingImprovements: 0
D4 table contains 527800 parameters.
A/D table contains 62925 parameters.
A/D table contains 63084 parameters.
NTable contains 534250 parameter.
p0_count is 2.95682e+06 and p1 is 110355; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.90229 PERPLEXITY 119.618
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 6.98758 PERPLEXITY 126.903

Model4 Viterbi Iteration : 5 took: 188 seconds

---------------------
Model4: Iteration 6
Reading more sentence pairs into memory ... 
10000
20000
30000
40000
50000
Reading more sentence pairs into memory ... 
[sent:100000]
60000
70000
80000
90000
100000
Reading more sentence pairs into memory ... 
110000
120000
130000
140000
150000
Reading more sentence pairs into memory ... 
160000
170000
Reading more sentence pairs into memory ... 
#centers(pre/hillclimbed/real): 1 1 1  #al: 608.248 #alsophisticatedcountcollection: 65.6344 #hcsteps: 3.78092
#peggingImprovements: 0
D4 table contains 527800 parameters.
A/D table contains 62925 parameters.
A/D table contains 63084 parameters.
NTable contains 534250 parameter.
p0_count is 2.95728e+06 and p1 is 110123; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.72651 PERPLEXITY 105.897
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 6.80371 PERPLEXITY 111.718

Model4 Viterbi Iteration : 6 took: 143 seconds
H333444 Training Finished at: Sat Nov  5 21:42:22 2022


Entire Viterbi H333444 Training took: 661 seconds
==========================================================

Entire Training took: 1427 seconds
Program Finished at: Sat Nov  5 21:42:22 2022

==========================================================
Executing: rm -f /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.A3.final.gz
Executing: gzip /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.A3.final
(3) generate word alignment @ Sat Nov  5 21:42:27 CST 2022
Combining forward and inverted alignment from files:
  /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.A3.final.{bz2,gz}
  /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.A3.final.{bz2,gz}
Executing: mkdir -p /home/wujunchao/mosesdecoder/corpus/working/train/model
Executing: /home/wujunchao/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/wujunchao/mosesdecoder/corpus/working/train/giza.en-zh/en-zh.A3.final.gz" -i "gzip -cd /home/wujunchao/mosesdecoder/corpus/working/train/giza.zh-en/zh-en.A3.final.gz" |/home/wujunchao/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/wujunchao/mosesdecoder/corpus/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<173918>
(4) generate lexical translation table 0-0 @ Sat Nov  5 21:42:47 CST 2022
(/home/wujunchao/mosesdecoder/corpus/clean_train.zh,/home/wujunchao/mosesdecoder/corpus/clean_train.en,/home/wujunchao/mosesdecoder/corpus/working/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.f2e and /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.e2f
FILE: /home/wujunchao/mosesdecoder/corpus/clean_train.en
FILE: /home/wujunchao/mosesdecoder/corpus/clean_train.zh
FILE: /home/wujunchao/mosesdecoder/corpus/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Sat Nov  5 21:43:03 CST 2022
/home/wujunchao/mosesdecoder/scripts/generic/extract-parallel.perl 1 split "sort    " /home/wujunchao/mosesdecoder/scripts/../bin/extract /home/wujunchao/mosesdecoder/corpus/clean_train.en /home/wujunchao/mosesdecoder/corpus/clean_train.zh /home/wujunchao/mosesdecoder/corpus/working/train/model/aligned.grow-diag-final-and /home/wujunchao/mosesdecoder/corpus/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/wujunchao/mosesdecoder/scripts/generic/extract-parallel.perl 1 split "sort    " /home/wujunchao/mosesdecoder/scripts/../bin/extract /home/wujunchao/mosesdecoder/corpus/clean_train.en /home/wujunchao/mosesdecoder/corpus/clean_train.zh /home/wujunchao/mosesdecoder/corpus/working/train/model/aligned.grow-diag-final-and /home/wujunchao/mosesdecoder/corpus/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sat Nov  5 21:43:03 2022
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916; ls -l /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916 
total=173918 line-per-split=173919 
merging extract / extract.inv
gunzip -c /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916/extract.0000000.inv.gz  | LC_ALL=C sort     -T /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916 2>> /dev/stderr | gzip -c > /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916/extract.0000000.o.gz  | LC_ALL=C sort     -T /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916 2>> /dev/stderr | gzip -c > /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
gunzip -c /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916/extract.0000000.gz  | LC_ALL=C sort     -T /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6916 2>> /dev/stderr | gzip -c > /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.sorted.gz 2>> /dev/stderr 
Finished Sat Nov  5 21:46:26 2022
(6) score phrases @ Sat Nov  5 21:46:26 CST 2022
(6.1)  creating table half /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.f2e @ Sat Nov  5 21:46:26 CST 2022
/home/wujunchao/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /home/wujunchao/mosesdecoder/scripts/../bin/score /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.sorted.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.f2e /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/wujunchao/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /home/wujunchao/mosesdecoder/scripts/../bin/score /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.sorted.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.f2e /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Sat Nov  5 21:46:26 2022
ln -s /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.sorted.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6968/extract.0.gz 
/home/wujunchao/mosesdecoder/scripts/../bin/score /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6968/extract.0.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.f2e /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6968/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6968/run.0.shmv /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6968/phrase-table.half.0000000.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.f2e.gzrm -rf /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6968 
Finished Sat Nov  5 21:48:25 2022
(6.3)  creating table half /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.e2f @ Sat Nov  5 21:48:25 CST 2022
/home/wujunchao/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /home/wujunchao/mosesdecoder/scripts/../bin/score /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.inv.sorted.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.e2f /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/wujunchao/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /home/wujunchao/mosesdecoder/scripts/../bin/score /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.inv.sorted.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.e2f /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Sat Nov  5 21:48:25 2022
ln -s /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.inv.sorted.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6987/extract.0.gz 
/home/wujunchao/mosesdecoder/scripts/../bin/score /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6987/extract.0.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/lex.e2f /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6987/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6987/run.0.shgunzip -c /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6987/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6987  | gzip -c > /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/wujunchao/mosesdecoder/corpus/working/train/model/tmp.6987 
Finished Sat Nov  5 21:51:00 2022
(6.6) consolidating the two halves @ Sat Nov  5 21:51:00 CST 2022
Executing: /home/wujunchao/mosesdecoder/scripts/../bin/consolidate /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.f2e.gz /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
..............................................................................
Executing: rm -f /home/wujunchao/mosesdecoder/corpus/working/train/model/phrase-table.half.*
(7) learn reordering model @ Sat Nov  5 21:52:30 CST 2022
(7.1) [no factors] learn reordering model @ Sat Nov  5 21:52:30 CST 2022
(7.2) building tables @ Sat Nov  5 21:52:30 CST 2022
Executing: /home/wujunchao/mosesdecoder/scripts/../bin/lexical-reordering-score /home/wujunchao/mosesdecoder/corpus/working/train/model/extract.o.sorted.gz 0.5 /home/wujunchao/mosesdecoder/corpus/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sat Nov  5 21:53:13 CST 2022
  no generation model requested, skipping step
(9) create moses.ini @ Sat Nov  5 21:53:13 CST 2022
